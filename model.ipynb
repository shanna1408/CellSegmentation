{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.segmentation import slic\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellSegmentation():\n",
    "    def __init__(self, root, filenames, k_neighbors=5):\n",
    "\n",
    "        self.filenames = filenames\n",
    "        # Extract image paths and labels from the CSV\n",
    "        self.cellpaths = [os.path.join(f'{root}/Tissue Images', f'{filename}.tif') for filename in filenames]\n",
    "        self.maskpaths = [os.path.join(f'{root}/Masks', f'{filename}.npz') for filename in filenames]\n",
    "        self.k_neighbors = k_neighbors\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "\n",
    "        # Load and Resize Image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Flatten the Image (N_pixels, C)\n",
    "        img_flattened = image.reshape(-1, image.shape[-1])  # (50176, Channels)\n",
    "\n",
    "        # Standardize by removing mean and scaling to unit variance \n",
    "        scaler = StandardScaler()\n",
    "        embedding_standardized = scaler.fit_transform(img_flattened)\n",
    "\n",
    "        return embedding_standardized.reshape(image.shape[0], image.shape[1], -1)\n",
    "\n",
    "    # Load ground truth mask\n",
    "    def preprocess_mask(self, mask_path):\n",
    "        loaded_data = np.load(mask_path)\n",
    "        loaded_color_mask = loaded_data['color_mask']\n",
    "\n",
    "        loaded_color_mask = np.array(loaded_color_mask)\n",
    "        return loaded_color_mask\n",
    "    \n",
    "    # Generate a graph from the image features and the provided mask.\n",
    "    def generate_graph(self, features, mask):        \n",
    "        # Superpixel segmentation\n",
    "        segments = slic(features, n_segments=1000, compactness=15, start_label=0)\n",
    "        nodes = np.unique(segments)  # Get unique segment labels\n",
    "        node_features = []\n",
    "        node_labels = []\n",
    "\n",
    "        for node in nodes:\n",
    "            mask_node = segments == node\n",
    "            mean_features = features[mask_node].mean(axis=0)\n",
    "            node_features.append(mean_features)\n",
    "\n",
    "            superpixel_mask_values = mask[mask_node]\n",
    "            unique, counts = np.unique(superpixel_mask_values, return_counts=True)\n",
    "            node_label = unique[np.argmax(counts)]  # Assign most frequent class in superpixel\n",
    "            node_labels.append(node_label)\n",
    "\n",
    "        node_features = np.array(node_features)\n",
    "        node_labels = np.array(node_labels)\n",
    "\n",
    "        # Construct adjacency matrix (k-NN or spatial would require understanding the form)\n",
    "        adj_matrix = kneighbors_graph(node_features, n_neighbors=self.k_neighbors).toarray()\n",
    "\n",
    "        # PyTorch conversion for the graph\n",
    "        edge_indices = np.array(np.nonzero(adj_matrix))\n",
    "        edge_indices = torch.tensor(edge_indices, dtype=torch.long)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        y = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_indices, y=y)\n",
    "    \n",
    "    # Generate a PyG dataset from image and mask paths.\n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            img_path = self.cellpaths[i]\n",
    "            mask_path = self.maskpaths[i]\n",
    "            features = self.preprocess_image(img_path)\n",
    "            mask = self.preprocess_mask(mask_path)\n",
    "\n",
    "            graph = self.generate_graph(features, mask)\n",
    "            dataset.append(graph)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        GNN Model with both node-level and graph-level classification.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension for each node.\n",
    "            hidden_dim (int): Hidden layer dimension.\n",
    "            output_dim (int): Output dimension for node-level classification..\n",
    "        \"\"\"\n",
    "        super(GNNModel, self).__init__()\n",
    "\n",
    "        # GNN layers: GCN and GAT for feature propagation\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=2, concat=False)\n",
    "        self.dropout2 = nn.Dropout(p=0.6)\n",
    "\n",
    "        # Node-level classification branch\n",
    "        self.node_classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass for GNN model.\n",
    "\n",
    "        Args:\n",
    "            data: A PyTorch Geometric Data object containing:\n",
    "                - data.x: Node features (N_nodes x input_dim)\n",
    "                - data.edge_index: Edge list (2 x N_edges)\n",
    "        Returns:\n",
    "            node_predictions (torch.Tensor): Node-level predictions (N_nodes x output_dim)\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.dropout1(F.relu(self.gcn1(x, edge_index)))\n",
    "        x = self.dropout2(F.relu(self.gat1(x, edge_index)))\n",
    "        \n",
    "        node_predictions = self.node_classifier(x)\n",
    "        return node_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set and calculate node-level and graph-level accuracy and Jaccard score.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "    \n",
    "    Returns:\n",
    "        node_jaccard: Jaccard accuracy score for node-level classification.\n",
    "        graph_jaccard: Jaccard accuracy score for graph-level classification.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    node_preds_all = []\n",
    "    node_labels_all = []\n",
    "\n",
    "    with torch.no_grad():  # No gradient computation during evaluation\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)  # Move data to the device (GPU/CPU)\n",
    "\n",
    "            # Forward pass\n",
    "            node_predictions = model(data)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            node_preds_all.append(node_predictions.cpu().numpy())\n",
    "            node_labels_all.append(data.y.cpu().numpy())\n",
    "\n",
    "    # Flatten the lists for evaluation\n",
    "    node_preds_all = np.concatenate(node_preds_all, axis=0)\n",
    "    node_labels_all = np.concatenate(node_labels_all, axis=0)\n",
    "\n",
    "    # Calculate Jaccard score for node-level and graph-level classification\n",
    "    jaccard = jaccard_score(node_labels_all, node_preds_all.argmax(axis=1), average='macro')  # Macro for multi-class\n",
    "\n",
    "    return jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5, lr=1e-3, patience=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate the GNN model for a given number of epochs. Includes early stopping and learning rate adjustment.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model.\n",
    "        train_loader: DataLoader for the training set.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        patience (int): Number of epochs to wait for performance improvement before stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU or CPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    best_node_accuracy = 0.0  # To track the best node-level accuracy\n",
    "    patience_counter = 0  # To track the number of epochs without improvement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)  # Move data to GPU/CPU\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            # Forward pass\n",
    "            node_predictions = model(data)\n",
    "            # Loss calculation\n",
    " \n",
    "            node_loss = criterion(node_predictions, data.y)  # Node-level loss\n",
    "            # Total loss\n",
    "            total_loss = node_loss\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "        print(f\"Training Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        node_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Node-level Accuracy: {node_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping check and learning rate adjustment\n",
    "        if node_accuracy > best_node_accuracy:\n",
    "            torch.save(model, 'best_model.pth')\n",
    "            best_node_accuracy = node_accuracy\n",
    "            patience_counter = 0  # Reset patience counter if performance improves\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. No improvement in accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Best Node Accuracy so far: {best_node_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "# Specify the directory\n",
    "directory = Path(\"Dataset/MoNuSeg 2018 Training Data/Annotations\")\n",
    "\n",
    "# Loop through every file in the folder\n",
    "for file_path in directory.iterdir():\n",
    "    # Check if it's a file (not a directory)\n",
    "    if file_path.is_file():\n",
    "        # Get the file name without the extension\n",
    "        file_name_without_extension = file_path.stem\n",
    "        filenames.append(file_name_without_extension)\n",
    "\n",
    "# Initialize class and preprocess data\n",
    "segmentation = CellSegmentation(root=\"Dataset/MoNuSeg 2018 Training Data\", filenames=filenames)\n",
    "dataset = segmentation.create_dataset()\n",
    "print(len(dataset))\n",
    "# Split into train/test sets\n",
    "train_loader = DataLoader(dataset[:int(0.6 * len(dataset))], batch_size=6, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(0.4 * len(dataset)):], batch_size=6)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA enabled\")\n",
    "else:\n",
    "    print(\"CUDA not found\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3  # Node feature dimension is 1\n",
    "hidden_dim = 64\n",
    "output_dim = 3  # 3 classes for node-level classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Training Loss: 1.0780\n",
      "Node-level Accuracy: 0.0732\n",
      "Best Node Accuracy so far: 0.0732\n",
      "Epoch 2/15\n",
      "Training Loss: 1.0115\n",
      "Node-level Accuracy: 0.2931\n",
      "Best Node Accuracy so far: 0.2931\n",
      "Epoch 3/15\n",
      "Training Loss: 0.9528\n",
      "Node-level Accuracy: 0.4254\n",
      "Best Node Accuracy so far: 0.4254\n",
      "Epoch 4/15\n",
      "Training Loss: 0.8765\n",
      "Node-level Accuracy: 0.4977\n",
      "Best Node Accuracy so far: 0.4977\n",
      "Epoch 5/15\n",
      "Training Loss: 0.7844\n",
      "Node-level Accuracy: 1.0000\n",
      "Best Node Accuracy so far: 1.0000\n",
      "Epoch 6/15\n",
      "Training Loss: 0.6887\n",
      "Node-level Accuracy: 1.0000\n",
      "Best Node Accuracy so far: 1.0000\n",
      "Epoch 7/15\n",
      "Training Loss: 0.5830\n",
      "Node-level Accuracy: 1.0000\n",
      "Best Node Accuracy so far: 1.0000\n",
      "Epoch 8/15\n",
      "Training Loss: 0.5009\n",
      "Node-level Accuracy: 1.0000\n",
      "Best Node Accuracy so far: 1.0000\n",
      "Epoch 9/15\n",
      "Training Loss: 0.4086\n",
      "Node-level Accuracy: 1.0000\n",
      "Best Node Accuracy so far: 1.0000\n",
      "Epoch 10/15\n",
      "Training Loss: 0.3074\n",
      "Node-level Accuracy: 1.0000\n",
      "Early stopping at epoch 10. No improvement in accuracy for 5 epochs.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "#Init model\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-3)\n",
    "train(model, train_loader, test_loader, optimizer, device, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "testfiles = []\n",
    "# Specify the directory\n",
    "directory = Path(\"Dataset/MoNuSegTestData/Annotations\")\n",
    "\n",
    "# Loop through every file in the folder\n",
    "for file_path in directory.iterdir():\n",
    "    # Check if it's a file (not a directory)\n",
    "    if file_path.is_file():\n",
    "        # Get the file name without the extension\n",
    "        file_name_without_extension = file_path.stem\n",
    "        testfiles.append(file_name_without_extension)\n",
    "\n",
    "test = CellSegmentation(root=\"Dataset/MoNuSegTestData\", filenames=testfiles)\n",
    "dataset = test.create_dataset()\n",
    "print(len(dataset))\n",
    "# Split into train/test sets\n",
    "testset_loader = DataLoader(dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shanna\\AppData\\Local\\Temp\\ipykernel_20300\\150930735.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load('best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('best_model.pth')\n",
    "results = evaluate(best_model, testset_loader, device)\n",
    "print(f\"Accuracy: {results:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
