{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.io import imread\n",
    "from skimage.segmentation import slic\n",
    "from skimage.transform import resize\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellSegmentation():\n",
    "    def __init__(self, root, filenames, transforms=None, k_neighbors=8):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.filenames = filenames\n",
    "        # Extract image paths and labels from the CSV\n",
    "        self.cellpaths = [os.path.join(f'{root}/Tissue Images', f'{filename}.tif') for filename in filenames]\n",
    "        self.maskpaths = [os.path.join(f'{root}/Masks', f'{filename}.npz') for filename in filenames]\n",
    "        self.k_neighbors = k_neighbors\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "\n",
    "        # Load and Resize Image\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((224, 224))  \n",
    "        image = np.array(image)\n",
    "\n",
    "        # Flatten the Image (N_pixels, C)\n",
    "        img_flattened = image.reshape(-1, image.shape[-1])  # (50176, Channels)\n",
    "\n",
    "        # Standardize by removing mean and scaling to unit variance \n",
    "        scaler = StandardScaler()\n",
    "        embedding_standardized = scaler.fit_transform(img_flattened)\n",
    "\n",
    "        # PCA for feature reduction\n",
    "        pca = PCA(n_components=1)  \n",
    "        reduced_embedding = pca.fit_transform(embedding_standardized)\n",
    "\n",
    "        return reduced_embedding.reshape(image.shape[0], image.shape[1], -1), pca\n",
    "\n",
    "    # Load ground truth binary mask\n",
    "    def preprocess_mask(self, mask_path):\n",
    "        loaded_data = np.load(mask_path)\n",
    "        loaded_binary_mask = loaded_data['color_mask']\n",
    "\n",
    "        # Resize the mask to match the image shape (224, 224)\n",
    "        resized_mask = resize(loaded_binary_mask, (224, 224), mode='reflect', anti_aliasing=True)\n",
    "\n",
    "        return resized_mask\n",
    "    \n",
    "    # Generate a graph from the image features and the provided binary mask.\n",
    "    def generate_graph(self, features, mask):        \n",
    "        # Superpixel segmentation\n",
    "        segments = slic(features, n_segments=500, compactness=10, start_label=0)\n",
    "        nodes = np.unique(segments)  # Get unique segment labels\n",
    "        node_features = []\n",
    "        node_labels = []\n",
    "\n",
    "        for node in nodes:\n",
    "            # Aggregate features for each superpixel\n",
    "            mask_node = segments == node\n",
    "            mean_features = features[mask_node].mean(axis=0)  # Mean of PCA features for superpixel\n",
    "            node_features.append(mean_features)\n",
    "\n",
    "            # Compute label for superpixel using binary mask\n",
    "            superpixel_mask_values = mask[mask_node]  # Mask values for pixels in superpixel\n",
    "            mean_label = superpixel_mask_values.mean()  # Average value in binary mask\n",
    "            node_label = 1 if mean_label > 0.5 else 0  # Threshold mean to produce a binary label\n",
    "            node_labels.append(node_label)\n",
    "\n",
    "        # Convert lists to arrays\n",
    "        node_features = np.array(node_features)  # Shape: (N_nodes, PCA_components)\n",
    "        node_labels = np.array(node_labels)  # Shape: (N_nodes,)\n",
    "\n",
    "        # Step 2: Build adjacency matrix (spatially or by k-NN)\n",
    "        adj_matrix = kneighbors_graph(\n",
    "            node_features, n_neighbors=self.k_neighbors, mode=\"connectivity\"\n",
    "        ).toarray()\n",
    "\n",
    "        # Step 3: Build PyG graph\n",
    "        edge_indices = np.array(np.nonzero(adj_matrix))  # (2, num_edges)\n",
    "        edge_indices = torch.tensor(edge_indices, dtype=torch.long)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        y = torch.tensor(node_labels, dtype=torch.float)  # Node labels from the binary mask\n",
    "\n",
    "        return Data(x=x, edge_index=edge_indices, y=y)\n",
    "    \n",
    "    # Generate a PyG dataset from image and mask paths.\n",
    "    def create_dataset(self):\n",
    "        dataset = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            img_path = self.cellpaths[i]\n",
    "            mask_path = self.maskpaths[i]\n",
    "            features, _ = self.preprocess_image(img_path)\n",
    "            mask = self.preprocess_mask(mask_path)\n",
    "\n",
    "            graph = self.generate_graph(features, mask)\n",
    "            dataset.append(graph)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, graph_embedding_dim):\n",
    "        \"\"\"\n",
    "        GNN Model with both node-level and graph-level classification.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input feature dimension for each node.\n",
    "            hidden_dim (int): Hidden layer dimension.\n",
    "            output_dim (int): Output dimension for node-level classification.\n",
    "            graph_embedding_dim (int): Output dimension for graph-level embedding.\n",
    "        \"\"\"\n",
    "        super(GNNModel, self).__init__()\n",
    "\n",
    "        # GNN layers: GCN and GAT for feature propagation\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=4, concat=False)  # Multi-head attention\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node-level classification branch\n",
    "        self.node_classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass for GNN model.\n",
    "\n",
    "        Args:\n",
    "            data: A PyTorch Geometric Data object containing:\n",
    "                - data.x: Node features (N_nodes x input_dim)\n",
    "                - data.edge_index: Edge list (2 x N_edges)\n",
    "                - data.batch: Batch indices for graph-level pooling\n",
    "\n",
    "        Returns:\n",
    "            node_predictions (torch.Tensor): Node-level predictions (N_nodes x output_dim)\n",
    "            graph_predictions (torch.Tensor): Graph-level predictions (batch_size x graph_embedding_dim)\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # GNN layers for feature propagation\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Node-level classification\n",
    "        node_predictions = torch.sigmoid(self.node_classifier(x))\n",
    "\n",
    "        return node_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set and calculate node-level and graph-level accuracy and Jaccard score.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "    \n",
    "    Returns:\n",
    "        node_jaccard: Jaccard accuracy score for node-level classification.\n",
    "        graph_jaccard: Jaccard accuracy score for graph-level classification.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    node_preds_all = []\n",
    "    node_labels_all = []\n",
    "\n",
    "    with torch.no_grad():  # No gradient computation during evaluation\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)  # Move data to the device (GPU/CPU)\n",
    "\n",
    "            # Forward pass\n",
    "            node_predictions = model(data)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            node_preds_all.append(node_predictions.cpu().numpy())\n",
    "            node_labels_all.append(data.y.cpu().numpy())\n",
    "\n",
    "    # Flatten the lists for evaluation\n",
    "    node_preds_all = np.concatenate(node_preds_all, axis=0)\n",
    "    node_labels_all = np.concatenate(node_labels_all, axis=0)\n",
    "\n",
    "    # Calculate Jaccard score for node-level and graph-level classification\n",
    "    # For Jaccard score, consider the binary classification (0 or 1) for each label\n",
    "    jaccard = jaccard_score(node_labels_all, node_preds_all.argmax(axis=1), average='macro')  # Macro for multi-class\n",
    "\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5, lr=1e-3, patience=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate the GNN model for a given number of epochs. Includes early stopping and learning rate adjustment.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model.\n",
    "        train_loader: DataLoader for the training set.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        patience (int): Number of epochs to wait for performance improvement before stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU or CPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    best_node_accuracy = 0.0  # To track the best node-level accuracy\n",
    "    patience_counter = 0  # To track the number of epochs without improvement\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)  # Move data to GPU/CPU\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            # Forward pass\n",
    "            node_predictions = model(data)\n",
    "            # Loss calculation\n",
    " \n",
    "            node_loss = criterion(node_predictions, data.y.long())  # Node-level loss\n",
    "            # Total loss\n",
    "            total_loss = node_loss\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "        print(f\"Training Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        node_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Node-level Accuracy: {node_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping check and learning rate adjustment\n",
    "        if node_accuracy > best_node_accuracy:\n",
    "            best_node_accuracy = node_accuracy\n",
    "            patience_counter = 0  # Reset patience counter if performance improves\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. No improvement in accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Best Node Accuracy so far: {best_node_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484]), Data(x=[484, 1], edge_index=[2, 3872], y=[484])]\n",
      "<torch_geometric.loader.dataloader.DataLoader object at 0x0000018A6EC9B140>\n",
      "CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "# Specify the directory\n",
    "directory = Path(\"Dataset/MoNuSeg 2018 Training Data/Annotations\")\n",
    "\n",
    "# Loop through every file in the folder\n",
    "for file_path in directory.iterdir():\n",
    "    # Check if it's a file (not a directory)\n",
    "    if file_path.is_file():\n",
    "        # Get the file name without the extension\n",
    "        file_name_without_extension = file_path.stem\n",
    "        filenames.append(file_name_without_extension)\n",
    "\n",
    "# Initialize class and preprocess data\n",
    "segmentation = CellSegmentation(root=\"Dataset/MoNuSeg 2018 Training Data\", filenames=filenames)\n",
    "dataset = segmentation.create_dataset()\n",
    "print(dataset)\n",
    "# Split into train/test sets\n",
    "train_loader = DataLoader(dataset[:int(0.8 * len(dataset))], batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(0.8 * len(dataset)):], batch_size=4)\n",
    "print(train_loader)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA enabled\")\n",
    "else:\n",
    "    print(\"CUDA not found\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1  # Node feature dimension is 1\n",
    "hidden_dim = 64\n",
    "output_dim = 3  # 3 classes for node-level classification\n",
    "graph_embedding_dim = 5  # 5 categories for graph-level classification\n",
    "\n",
    "#Init model\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim, graph_embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training Loss: 1.0646\n",
      "Node-level Accuracy: 0.4996\n",
      "Best Node Accuracy so far: 0.4996\n",
      "Epoch 2/5\n",
      "Training Loss: 1.0199\n",
      "Node-level Accuracy: 0.4996\n",
      "Best Node Accuracy so far: 0.4996\n",
      "Epoch 3/5\n",
      "Training Loss: 0.9557\n",
      "Node-level Accuracy: 0.4996\n",
      "Best Node Accuracy so far: 0.4996\n",
      "Epoch 4/5\n",
      "Training Loss: 0.8697\n",
      "Node-level Accuracy: 0.4996\n",
      "Best Node Accuracy so far: 0.4996\n",
      "Epoch 5/5\n",
      "Training Loss: 0.7111\n",
      "Node-level Accuracy: 0.4996\n",
      "Best Node Accuracy so far: 0.4996\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, optimizer, device, epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
